Detecting AI-Generated Text with Naive Bayes Classifier
In this blog post, we will explore the process of building a Naive Bayes Classifier (NBC) to detect whether 
a given essay is written by a human or generated by an AI model. We will walk through the steps of data 
preparation, model training, and evaluation. This work is based on the "2023 Fall Data Mining 
Assignment #2."
Introduction
The ability of artificial intelligence (AI) to produce text that is human-like has advanced significantly. But 
it's crucial to understand the difference between content produced by humans and artificial intelligence, 
particularly for applications like plagiarism detection and content moderation. We can solve this issue 
with the aid of the straightforward but effective machine learning algorithm known as the Naive Bayes 
Classifier.
Step 1: Data Collection
We require a dataset of essays that are classified as "human-written" or "AI-generated" in order to train 
our classifier. Our dataset was acquired through the "LLM Detect AI-Generated Text" competition on 
Kaggle [1]. Essays in this dataset are categorized as either 0 (written by humans) or 1 (generated by 
artificial intelligence).
Step 2: Data Preprocessing
Prior to training our NBC, the text data needs to be preprocessed. This comprises:
• Lowercasing: To maintain consistency, all text should be converted to lowercase.
• Eliminating Punctuation: Taking out all punctuation from the text.
• Tokenization is the process of dividing the text into discrete words or units.
For these text preprocessing tasks, we made use of the NLTK library in Python.
Step 3: Data Splitting
We divided the dataset into two sets, a training set comprising 80% of the data and a development set 
comprising 20% of the data, in order to assess our classifier. To make sure that both sets are 
representative of the whole dataset, we employed random shuffling.
Step 4: Building the Vocabulary
After that, we created a vocabulary using the training data. Every distinct word that appears in the 
training data at least five times is included in the vocabulary. In order to facilitate effective computations, 
we also developed a reverse index, which maps words to numerical indices.
Step 5: Calculating Probabilities
Using the training data and our vocabulary, we computed two different kinds of probabilities:
• Word Occurrence Probability: The likelihood that a word will appear in any given essay.
• The likelihood of a word occurring given the class (human or AI-generated) is known as 
conditional probability.
The Naive Bayes Classifier relies heavily on these probabilities.
Step 6: Naive Bayes Classifier
Our Naive Bayes classifier function can categorize essays into two groups: human- and AI-generated. We 
assessed the accuracy of the classifier using the development set, which functions as a validation set.
Step 7: Experimentation
We experimented with Laplace smoothing to enhance the performance of the classifier. Smoothing 
improves the model's generalization and helps avoid zero probabilities.
We also determined the top ten words for each class prediction. Knowing these terms can help you 
understand what makes text produced by AI different from content written by humans.
Step 8: Preparing for Submission
Finally, we prepared our classifier for submission. This includes smoothing probabilities and identifying 
the top words for both classes.
Conclusion
We've gone through the steps of creating a Naive Bayes classifier to identify text produced by artificial 
intelligence in this blog post. Data gathering, preprocessing, splitting, vocabulary building, probability 
computation, classification, and experimentation have all been discussed. For many applications, it is 
necessary to comprehend the distinctions between text generated by AI and human authors. The Naive 
Bayes Classifier offers a workable solution in this regard.
Reference
[1] Kaggle's "LLM Detect AI-Generated Text" competition dataset. Retrieved from 
https://www.kaggle.com/competitions/llm-detect-ai-generated-text
[2] Bird, S., Loper, E., & Klein, E. (2009). Natural Language Processing with Python: Analyzing Text with 
the Natural Language Toolkit. O'Reilly Media. https://www.nltk.org/book/
[3] Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to Information Retrieval. Cambridge 
University Press. https://nlp.stanford.edu/IR-book/
[4] Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). Tackling the poor assumptions of Naive Bayes 
text classifiers. In Proceedings of the Twentieth International Conference on International Conference on 
Machine Learning (ICML'03) (Vol. 3). Paper Link
[5] Lewis, D. D. (1998). Naive (Bayes) at forty: The independence assumption in information retrieval. In 
European Conference on Machine Learning (ECML) (pp. 4-15). Springer. Paper Link
[6] Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Computing Surveys 
(CSUR), 34(1), 1-47. Paper Link
[7] McCallum, A., & Nigam, K. (1998). A comparison of event models for Naive Bayes text classification. 
In AAAI-98 workshop on learning for text categorization (Vol. 752, No. 1, p. 41). Madison, WI. Paper Link
