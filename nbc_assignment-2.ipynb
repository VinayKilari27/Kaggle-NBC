{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 61542,
          "databundleVersionId": 6888007,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# step1:Load and Prepare the Data\n",
        "import csv\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Function to load data from a CSV file\n",
        "def load_data(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        headers = next(reader)\n",
        "        data = [row for row in reader]\n",
        "    return headers, data\n",
        "\n",
        "# Load training data\n",
        "train_headers, train_data = load_data('train_essays.csv')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ejqPYaaNcLJf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess the Text Data\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "# Preprocess essays in the training data\n",
        "for row in train_data:\n",
        "    text_index = train_headers.index('text')\n",
        "    row[text_index] = preprocess_text(row[text_index])"
      ],
      "metadata": {
        "trusted": true,
        "id": "GzWkJ9tPcLJj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split the Data into Training and Development Sets bold text\n",
        "# Split the data manually\n",
        "def split_data(data, split_ratio=0.8):\n",
        "    random.shuffle(data)\n",
        "    split_point = int(len(data) * split_ratio)\n",
        "    return data[:split_point], data[split_point:]\n",
        "\n",
        "train_data, dev_data = split_data(train_data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5foghuwjcLJk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Build the Vocabulary and the Reverse Index\n",
        "# Function to build vocabulary and reverse index\n",
        "def build_vocabulary_and_index(data):\n",
        "    word_counts = Counter(word for row in data for word in row[train_headers.index('text')])\n",
        "    vocabulary = {word for word, count in word_counts.items() if count >= 5}\n",
        "    reverse_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "    return vocabulary, reverse_index\n",
        "\n",
        "vocabulary, reverse_index = build_vocabulary_and_index(train_data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WP0ijU9ocLJk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Calculate Probabilities\n",
        "# Function to calculate word occurrence probabilities\n",
        "def calculate_probabilities(data, vocabulary):\n",
        "    word_occurrences = defaultdict(int)\n",
        "    class_word_occurrences = defaultdict(lambda: defaultdict(int))\n",
        "    class_counts = defaultdict(int)\n",
        "\n",
        "    for row in data:\n",
        "        label = int(row[train_headers.index('generated')])\n",
        "        class_counts[label] += 1\n",
        "        words = set(row[train_headers.index('text')])\n",
        "        for word in words:\n",
        "            if word in vocabulary:\n",
        "                word_occurrences[word] += 1\n",
        "                class_word_occurrences[label][word] += 1\n",
        "\n",
        "    total_docs = len(data)\n",
        "    word_probs = {word: count / total_docs for word, count in word_occurrences.items()}\n",
        "    word_given_class_probs = {\n",
        "        label: {word: (count / class_counts[label]) for word, count in word_counts.items()}\n",
        "        for label, word_counts in class_word_occurrences.items()\n",
        "    }\n",
        "\n",
        "    return word_probs, word_given_class_probs\n",
        "\n",
        "word_probs, word_given_class_probs = calculate_probabilities(train_data, vocabulary)\n",
        "\n",
        "def calculate_ai_generated_prob(document, vocabulary, word_given_class_probs, class_probs):\n",
        "    doc_words = set(document)\n",
        "    ai_generated_score = class_probs[1]  # The probability of the AI-generated class\n",
        "    not_ai_generated_score = class_probs[0]  # The probability of the not-AI-generated class\n",
        "\n",
        "    for word in doc_words:\n",
        "        if word in vocabulary:\n",
        "            # Probability of word given AI-generated\n",
        "            ai_generated_score *= word_given_class_probs[1].get(word, 1e-10)\n",
        "            # Probability of word given not AI-generated\n",
        "            not_ai_generated_score *= word_given_class_probs[0].get(word, 1e-10)\n",
        "\n",
        "    # Calculate the normalized probability\n",
        "    total = ai_generated_score + not_ai_generated_score\n",
        "    ai_generated_prob = ai_generated_score / total if total > 0 else 0.5  # Avoid division by zero\n",
        "\n",
        "    return ai_generated_prob\n",
        "\n",
        "# Function to generate probabilities for the test data\n",
        "def generate_probabilities_for_test_data(data, calculate_prob_func, vocabulary, word_given_class_probs, class_probs):\n",
        "    probabilities = []\n",
        "    for row in data:\n",
        "        document = row[train_headers.index('text')]\n",
        "        prob = calculate_prob_func(document, vocabulary, word_given_class_probs, class_probs)\n",
        "        probabilities.append(prob)\n",
        "    return probabilities"
      ],
      "metadata": {
        "trusted": true,
        "id": "oGDl-E30cLJl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Define the Classifier and Evaluate Accuracy\n",
        "# Function to classify a new document\n",
        "def classify(document, vocabulary, reverse_index, word_given_class_probs):\n",
        "    doc_words = set(document)\n",
        "    class_scores = defaultdict(float)\n",
        "\n",
        "    for word in doc_words:\n",
        "        if word in vocabulary:\n",
        "            for class_label, probs in word_given_class_probs.items():\n",
        "                word_idx = reverse_index[word]\n",
        "                class_scores[class_label] += probs.get(word_idx, 0)\n",
        "\n",
        "    return max(class_scores, key=class_scores.get)\n",
        "\n",
        "# Function to evaluate the classifier\n",
        "def evaluate(data, classify_func):\n",
        "    correct = 0\n",
        "    for row in data:\n",
        "        label = int(row[train_headers.index('generated')])\n",
        "        prediction = classify_func(row[train_headers.index('text')], vocabulary, reverse_index, word_given_class_probs)\n",
        "        if prediction == label:\n",
        "            correct += 1\n",
        "    return correct / len(data)\n",
        "\n",
        "accuracy = evaluate(dev_data, classify)\n",
        "print(f\"Development Set Accuracy: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsSGu-c4cLJm",
        "outputId": "bc7e81f4-d886-4dd5-aba8-1081e7a39b38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Set Accuracy: 99.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Experiment with Smoothing and Identify Top Predictive Words\n",
        "def train_naive_bayes(data, vocabulary):\n",
        "    word_given_class_counts = defaultdict(lambda: defaultdict(int))\n",
        "    class_counts = defaultdict(int)\n",
        "\n",
        "    # Count how many times each word appears in documents of each class\n",
        "    for row in data:\n",
        "        label = int(row[train_headers.index('generated')])\n",
        "        class_counts[label] += 1\n",
        "        words = row[train_headers.index('text')]\n",
        "        for word in words:\n",
        "            if word in vocabulary:\n",
        "                word_given_class_counts[label][word] += 1\n",
        "\n",
        "    # Apply Laplace smoothing to word counts and convert them to probabilities\n",
        "    word_given_class_probs = {\n",
        "        label: {\n",
        "            word: (word_count + 1) / (sum(class_word_counts.values()) + len(vocabulary))\n",
        "            for word, word_count in class_word_counts.items()\n",
        "        } for label, class_word_counts in word_given_class_counts.items()\n",
        "    }\n",
        "\n",
        "    # Calculate class probabilities\n",
        "    total_docs = sum(class_counts.values())\n",
        "    class_probs = {label: count / total_docs for label, count in class_counts.items()}\n",
        "\n",
        "    return word_given_class_probs, class_probs\n",
        "\n",
        "# Run the training function\n",
        "word_given_class_probs, class_probs = train_naive_bayes(train_data, vocabulary)\n",
        "\n",
        "# Define the function to get top words with their probabilities\n",
        "def get_top_words(word_given_class_probs, vocabulary, top_n=10):\n",
        "    top_words = {}\n",
        "    for label, word_probs in word_given_class_probs.items():\n",
        "        # Get the top_n words by probability for each class\n",
        "        top_words[label] = sorted(word_probs.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
        "    return top_words\n",
        "\n",
        "# Get the top words for each class\n",
        "top_words = get_top_words(word_given_class_probs, vocabulary)\n",
        "\n",
        "# Now print the top words for each class\n",
        "for label, words in top_words.items():\n",
        "    print(f\"Class {label}:\")\n",
        "    for word, prob in words:\n",
        "        print(f\"  {word}: {prob:.6f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DPHI7g4cLJm",
        "outputId": "26cbf74c-05d8-4b1e-ecc2-6d63bb73808a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0:\n",
            "  the: 0.064804\n",
            "  to: 0.032030\n",
            "  of: 0.029471\n",
            "  a: 0.025067\n",
            "  and: 0.023211\n",
            "  in: 0.021758\n",
            "  is: 0.016714\n",
            "  that: 0.014802\n",
            "  for: 0.012570\n",
            "  it: 0.010691\n",
            "Class 1:\n",
            "  the: 0.019723\n",
            "  and: 0.014792\n",
            "  of: 0.012481\n",
            "  a: 0.011864\n",
            "  to: 0.011556\n",
            "  that: 0.007396\n",
            "  in: 0.006626\n",
            "  for: 0.005547\n",
            "  car: 0.005085\n",
            "  is: 0.004931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "test_headers, test_data = load_data('test_essays.csv')\n",
        "\n",
        "# Preprocess test data\n",
        "for row in test_data:\n",
        "    text_index = test_headers.index('text')\n",
        "    row[text_index] = preprocess_text(row[text_index])\n",
        "\n",
        "# Generate probabilities for the test data with probabilities\n",
        "test_probabilities = generate_probabilities_for_test_data(test_data, calculate_ai_generated_prob, vocabulary, word_given_class_probs, class_probs)\n",
        "\n",
        "# Save the probabilities to submission.csv\n",
        "def save_probabilities_to_csv(ids, probabilities, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"id\", \"generated\"])\n",
        "        for id, prob in zip(ids, probabilities):\n",
        "            writer.writerow([id, prob])\n",
        "\n",
        "# Extract the ids from the test dataset\n",
        "test_ids = [row[0] for row in test_data]  # Assuming the first column is 'id'\n",
        "\n",
        "# Save probabilities to submission.csv (adjust the path as needed)\n",
        "save_probabilities_to_csv(test_ids, test_probabilities, 'submission.csv')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "oDPPOFD4cLJn"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}