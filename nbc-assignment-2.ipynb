{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# step1:Load and Prepare the Data\nimport csv\nimport random\nfrom collections import defaultdict, Counter\n\n# Function to load data from a CSV file\ndef load_data(filename):\n    with open(filename, 'r', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        headers = next(reader)\n        data = [row for row in reader]\n    return headers, data\n\n# Load training data\ntrain_headers, train_data = load_data('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Preprocess the Text Data\nimport re\nimport string\n\n# Function to clean and tokenize text\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n    return text.split()\n\n# Preprocess essays in the training data\nfor row in train_data:\n    text_index = train_headers.index('text')\n    row[text_index] = preprocess_text(row[text_index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Split the Data into Training and Development Sets bold text\n# Split the data manually\ndef split_data(data, split_ratio=0.8):\n    random.shuffle(data)\n    split_point = int(len(data) * split_ratio)\n    return data[:split_point], data[split_point:]\n\ntrain_data, dev_data = split_data(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Build the Vocabulary and the Reverse Index\n# Function to build vocabulary and reverse index\ndef build_vocabulary_and_index(data):\n    word_counts = Counter(word for row in data for word in row[train_headers.index('text')])\n    vocabulary = {word for word, count in word_counts.items() if count >= 5}\n    reverse_index = {word: idx for idx, word in enumerate(vocabulary)}\n    return vocabulary, reverse_index\n\nvocabulary, reverse_index = build_vocabulary_and_index(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 5: Calculate Probabilities\n# Function to calculate word occurrence probabilities\ndef calculate_probabilities(data, vocabulary):\n    word_occurrences = defaultdict(int)\n    class_word_occurrences = defaultdict(lambda: defaultdict(int))\n    class_counts = defaultdict(int)\n    \n    for row in data:\n        label = int(row[train_headers.index('generated')])\n        class_counts[label] += 1\n        words = set(row[train_headers.index('text')])\n        for word in words:\n            if word in vocabulary:\n                word_occurrences[word] += 1\n                class_word_occurrences[label][word] += 1\n\n    total_docs = len(data)\n    word_probs = {word: count / total_docs for word, count in word_occurrences.items()}\n    word_given_class_probs = {\n        label: {word: (count / class_counts[label]) for word, count in word_counts.items()}\n        for label, word_counts in class_word_occurrences.items()\n    }\n    \n    return word_probs, word_given_class_probs\n\nword_probs, word_given_class_probs = calculate_probabilities(train_data, vocabulary)\n\ndef calculate_ai_generated_prob(document, vocabulary, word_given_class_probs, class_probs):\n    doc_words = set(document)\n    ai_generated_score = class_probs[1]  # The probability of the AI-generated class\n    not_ai_generated_score = class_probs[0]  # The probability of the not-AI-generated class\n    \n    for word in doc_words:\n        if word in vocabulary:\n            # Probability of word given AI-generated\n            ai_generated_score *= word_given_class_probs[1].get(word, 1e-10)  \n            # Probability of word given not AI-generated\n            not_ai_generated_score *= word_given_class_probs[0].get(word, 1e-10)\n    \n    # Calculate the normalized probability\n    total = ai_generated_score + not_ai_generated_score\n    ai_generated_prob = ai_generated_score / total if total > 0 else 0.5  # Avoid division by zero\n    \n    return ai_generated_prob\n\n# Function to generate probabilities for the test data\ndef generate_probabilities_for_test_data(data, calculate_prob_func, vocabulary, word_given_class_probs, class_probs):\n    probabilities = []\n    for row in data:\n        document = row[train_headers.index('text')]\n        prob = calculate_prob_func(document, vocabulary, word_given_class_probs, class_probs)\n        probabilities.append(prob)\n    return probabilities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6: Define the Classifier and Evaluate Accuracy\n# Function to classify a new document\ndef classify(document, vocabulary, reverse_index, word_given_class_probs):\n    doc_words = set(document)\n    class_scores = defaultdict(float)\n    \n    for word in doc_words:\n        if word in vocabulary:\n            for class_label, probs in word_given_class_probs.items():\n                word_idx = reverse_index[word]\n                class_scores[class_label] += probs.get(word_idx, 0)\n    \n    return max(class_scores, key=class_scores.get)\n\n# Function to evaluate the classifier\ndef evaluate(data, classify_func):\n    correct = 0\n    for row in data:\n        label = int(row[train_headers.index('generated')])\n        prediction = classify_func(row[train_headers.index('text')], vocabulary, reverse_index, word_given_class_probs)\n        if prediction == label:\n            correct += 1\n    return correct / len(data)\n\naccuracy = evaluate(dev_data, classify)\nprint(f\"Development Set Accuracy: {accuracy:.2%}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 7: Experiment with Smoothing and Identify Top Predictive Words\ndef train_naive_bayes(data, vocabulary):\n    word_given_class_counts = defaultdict(lambda: defaultdict(int))\n    class_counts = defaultdict(int)\n    \n    # Count how many times each word appears in documents of each class\n    for row in data:\n        label = int(row[train_headers.index('generated')])\n        class_counts[label] += 1\n        words = row[train_headers.index('text')]\n        for word in words:\n            if word in vocabulary:\n                word_given_class_counts[label][word] += 1\n    \n    # Apply Laplace smoothing to word counts and convert them to probabilities\n    word_given_class_probs = {\n        label: {\n            word: (word_count + 1) / (sum(class_word_counts.values()) + len(vocabulary))\n            for word, word_count in class_word_counts.items()\n        } for label, class_word_counts in word_given_class_counts.items()\n    }\n    \n    # Calculate class probabilities\n    total_docs = sum(class_counts.values())\n    class_probs = {label: count / total_docs for label, count in class_counts.items()}\n    \n    return word_given_class_probs, class_probs\n\n# Run the training function\nword_given_class_probs, class_probs = train_naive_bayes(train_data, vocabulary)\n\n# Define the function to get top words with their probabilities\ndef get_top_words(word_given_class_probs, vocabulary, top_n=10):\n    top_words = {}\n    for label, word_probs in word_given_class_probs.items():\n        # Get the top_n words by probability for each class\n        top_words[label] = sorted(word_probs.items(), key=lambda item: item[1], reverse=True)[:top_n]\n    return top_words\n\n# Get the top words for each class\ntop_words = get_top_words(word_given_class_probs, vocabulary)\n\n# Now print the top words for each class\nfor label, words in top_words.items():\n    print(f\"Class {label}:\")\n    for word, prob in words:\n        print(f\"  {word}: {prob:.6f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load test data\ntest_headers, test_data = load_data('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n\n# Preprocess test data\nfor row in test_data:\n    text_index = test_headers.index('text')\n    row[text_index] = preprocess_text(row[text_index])\n\n# Generate probabilities for the test data with probabilities\ntest_probabilities = generate_probabilities_for_test_data(test_data, calculate_ai_generated_prob, vocabulary, word_given_class_probs, class_probs)\n\n# Save the probabilities to submission.csv\ndef save_probabilities_to_csv(ids, probabilities, filename):\n    with open(filename, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"id\", \"generated\"])\n        for id, prob in zip(ids, probabilities):\n            writer.writerow([id, prob])\n\n# Extract the ids from the test dataset\ntest_ids = [row[0] for row in test_data]  # Assuming the first column is 'id'\n\n# Save probabilities to submission.csv (adjust the path as needed)\nsave_probabilities_to_csv(test_ids, test_probabilities, '/kaggle/working/submission.csv')\n\n# Confirm that submission.csv exists in the output directory\nimport os\nprint(os.listdir(\"/kaggle/working\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}